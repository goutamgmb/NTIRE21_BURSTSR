Table of contents 

# Introduction
Burst Image Super-Resolution Challenge will be held as part of the [6th edition of 
NTIRE: New Trends in Image Restoration and Enhancement](https://data.vision.ee.ethz.ch/cvl/ntire21/) workshop to be held in conjunction 
with [CVPR 2021](http://cvpr2021.thecvf.com/). The task of this challenge is to generate 
a denoised, demosaicked, higher-resolution image, given a RAW burst as input. 
The challenge uses a new dataset and has 2 tracks, namely **Track 1: Synthetic** and 
**Track 2: Real-world**. The top ranked participants in each track will be awarded and all 
participants are invited to submit a paper describing their solution to the associated 
NTIRE workshop at CVPR 2021


## Dates
* 2021.01.11 Release of train and validation data  
* 2021.01.15 Validation server online  
* 2021.03.01 Final test data release (inputs only)  
* 2021.03.08 Test output results submission deadline  
* 2021.03.09 Fact sheets and code/executable submission deadline  
* 2021.03.11 Preliminary test results released to the participants  
* 2021.03.28 Paper submission deadline for entries from the challenge  
* 2021.06.15 NTIRE workshop and challenges, results and award ceremony (CVPR 2021, Online)  

## Description
Given multiple noisy RAW images of a scene, the task in burst super-resolution is to 
predict a denoised higher-resolution RGB image by combining information from the 
multiple input frames. Concretely, the method will be provided a burst sequence 
containing 14 images, where each image contains the RAW sensor data from a bayer filter 
(RGGB) mosaic. The images in the burst have unknown offsets with respect to each other, 
and are corrupted by noise. The goal is to exploit the information from the multiple 
input images to predict a denoised, demosaicked RGB image having a 4 times higher 
resolution, compared to the input. The challenge will have two tracks, 
namely 1) Synthetic and 2) Real-world based on the source of the input data. The goal 
in both the tracks is to reconstruct the **original** image as well as possible, and 
not to artificially generate a plausible, visually pleasing image.





## Track 1 - Synthetic
In the synthetic track, the input bursts are generated from RGB images using a synthetic 
data generation pipeline. 

**Data generation:** The input sRGB image is first converted to linear sensor space 
using an inverse camera pipeline. A LR burst is then generated by applying random 
translations and rotations, followed by bilinear downsampling. The generated burst is 
then mosaicked and corrupted by random noise. 

**Training set:** We provide [code](datasets/synthetic_burst_train_set.py) to generate the synthetic 
bursts using any image dataset for training. Note that any image dataset except the 
test split of the [Zurich RAW to RGB dataset](http://people.ee.ethz.ch/~ihnatova/pynet.html#dataset) 
can be used to generate synthetic bursts for training.  

**Validation set:** The bursts in the validation set have been 
pre-generated with the [data generation code](datasets/synthetic_burst_train_set.py), 
using the RGB images from the test split of the 
[Zurich RAW to RGB dataset](http://people.ee.ethz.ch/~ihnatova/pynet.html#dataset). 


### Evaluation
The methods will be ranked using the fidelity (in terms of PSNR) with the high-resolution 
ground truth, i.e. the linear sensor space image used to generate the burst. The focus of 
the challenge is on learning to reconstruct the original high-resolution image, and not 
the subsequent post-processing. Hence, the PSNR computation will be computed in the 
linear sensor space, before post-processing steps such as color correction, 
white-balancing, gamma correction etc.


### Validation/submission (codlab etc) (info on test will follow)

## Track 2 - Real-world
This track deals with the problem of real-world burst super-resolution. For this purpose, 
we introduce a new dataset BurstSR consisting of real-world bursts.

**BurstSR dataset:** BurstSR consists of 200 RAW burst sequences, and 
corresponding high-resolution ground truths (a reference for the paper introducing the 
dataset will be made available soon). Each burst sequence contains 14 RAW images 
captured using a handheld smartphone camera. For each burst sequence, we also capture 
a high-resolution image using a DSLR camera mounted on a tripod to serve as ground truth. 
We extract 160x160 crops from the bursts to obtain a training set consisting of
--- crops, and a validation set consisting of --- crops. 

**Challenges:** Since the burst and ground 
truth images are captured using different cameras, there exists a spatial mis-alignment, 
as well as color mis-match between the images. Thus, in addition to designing network 
architectures, developing effective training strategies to utilize mis-aligned training 
data is a key challenge in this track.

### Evaluation
Due to the spatial and color mis-alignments between the input burst and the ground truth, 
it is difficult to estimate similarity between the network prediction and the ground 
truth. We introduce **AlignedPSNR** metric for this purpose. A user study will also be conducted as a complement to 
determine the final ranking on the test set.

**AlignedPSNR:** AlignedPSNR first spatially 
aligns the network prediction to the ground truth, using pixel-wise optical flow 
estimated using [PWC-Net](https://arxiv.org/abs/1709.02371). A linear color mapping between the input burst and the ground 
truth, modeled as a 3x3 color correction matrix, is then estimated and used to transform 
the spatially aligned network prediction to the same color space as the ground truth. 
Finally, PSNR is computed between the spatially aligned and color corrected network 
prediction and the ground truth. 

**User study:** The emphasis of the user study 
will be on which method can best reconstruct the **original** high-frequency details. The 
goal is thus not to generate more pleasing images by modifying the output color space 
or generating artificial high frequency content not existing in the high-resolution 
ground truth.

### Validation/submission

## Toolkit
We also provide a Python toolkit which includes the necessary data loading and 
evaluation scripts. The toolkit contains the following modules.

* [data_processing](data_processing): Contains the forward and inverse camera pipeline 
  employed in [“Unprocessing images for learned raw denoising”](https://arxiv.org/abs/1811.11127), 
  as well as the [script](data_processing/synthetic_burst_generation.py) to generate a 
  synthetic burst from a single RGB image.
* [datasets](datasets): Contains the PyTorch dataset classes useful for the challenge. 
    * [synthetic_burst_train_set](datasets/synthetic_burst_train_set.py) provides the SyntheticBurst dataset which generates synthetic bursts using any image dataset. 
    * [zurich_raw2rgb_dataset](datasets/zurich_raw2rgb_dataset.py) can be used to load 
      the RGB images Zurich RAW to RGB mapping dataset. This can be used along with SyntheticBurst dataset to generate synthetic bursts for training.  	
    * [synthetic_burst_val_set](datasets/synthetic_burst_val_set.py) can be used to load 
      the pre-generated synthetic validation set.
    * [burstsr_dataset](datasets/burstsr_dataset.py) provides the BurstSRDataset class which can be used to load the RAW bursts and high-resolution ground truths for the real-world track.
* [pwcnet](pwcnet): The code for the optical flow network [PWC-Net](https://arxiv.org/abs/1811.11127) 
  borrowed from [pytorch-pwc](https://github.com/sniklaus/pytorch-pwc).
* [scripts](scripts): Includes example scripts for using the SyntheticBurst and 
  BurstSRDataset datasets, as well as the AlignedPSNR metric. 
  Additionally, it also includes a script to download and unpack the BurstSR dataset.
* [utils](utils): Contains the [AlignedPSNR](utils/metrics.py) metric, as well as other utility functions.

**Installation:** The toolkit requires [PyTorch](https://pytorch.org/) and [OpenCV](https://opencv.org/) 
for track 1, and additionally [exifread](https://pypi.org/project/ExifRead/) and 
[cupy](https://cupy.dev/) for track 2. The necessary packages can be installed with 
[anaconda](https://www.anaconda.com/), using the [install.sh](install.sh) script. 


## Data
We provide the following data as part of the challenge. 

**Synthetic validation set:** The official validation set for track 1. The synthetic bursts are generated from the RGB images from the test split of the Zurich RAW to RGB mapping dataset. 
The dataset can be downloaded from [here]().

**BurstSR train and validation set:** The training and validation set for track 2. 
The dataset has been split into 10 parts and can be downloaded and unpacked using the 
[download_burstsr_dataset](scripts/download_burstsr_dataset.py) script. In case of issues with the script, the download links 
are available [here](burstsr_links.md).

**Zurich RAW to RGB mapping set:** The RGB images from the training split of the 
[Zurich RAW to RGB mapping dataset](http://people.ee.ethz.ch/~ihnatova/pynet.html#dataset) 
can be downloaded from [here](). These RGB images can be 
used to generate synthetic bursts for training using  the SyntheticBurst class.

## Issues and questions: 
In case of any questions about the challenge or the toolkit, feel free to open an issue on Github.

## Organizers
* [Goutam Bhat](https://goutamgmb.github.io/) (goutam.bhat@vision.ee.ethz.ch)
* [Martin Danelljan](https://martin-danelljan.github.io/) (martin.danelljan@vision.ee.ethz.ch)
* [Radu Timofte](http://people.ee.ethz.ch/~timofter/) (radu.timofte@vision.ee.ethz.ch)

## Terms and conditions
The terms and conditions for participating in the challenge are provided [here](terms_and_conditions.md)


## Acknowledgements
The toolkit uses the forward and inverse camera pipeline code from [unprocessing](https://github.com/timothybrooks/unprocessing),
as well as the PWC-Net code from [pytorch-pwc](https://github.com/sniklaus/pytorch-pwc).